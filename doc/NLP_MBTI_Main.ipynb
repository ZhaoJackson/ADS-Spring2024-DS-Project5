{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1: install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (from imblearn) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: xgboost in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/jacksonzhao/miniconda3/lib/python3.11/site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install imblearn\n",
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jacksonzhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jacksonzhao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jacksonzhao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model evaluation libraries\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Data balancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation, Input\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# Model serialization\n",
    "import pickle\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Ensure all needed class weights and additional sklearn tools\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0: data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/mbti_1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "INFP    1832\n",
       "INFJ    1470\n",
       "INTP    1304\n",
       "INTJ    1091\n",
       "ENTP     685\n",
       "ENFP     675\n",
       "ISTP     337\n",
       "ISFP     271\n",
       "ENTJ     231\n",
       "ISTJ     205\n",
       "ENFJ     190\n",
       "ISFJ     166\n",
       "ESTP      89\n",
       "ESFP      48\n",
       "ESFJ      42\n",
       "ESTJ      39\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Lemmatization and stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize all words\n",
    "def lemmatize_text(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_words = word_tokenize(review)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in token_words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(review):\n",
    "    tokens = word_tokenize(review)\n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# arrange text\n",
    "def clean_text(review):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', review) # Keeps only letters and spaces\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['posts'].apply(lemmatize_text).apply(remove_stopwords).apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Encode labels for MBTI Categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENFJ': 0, 'ENFP': 1, 'ENTJ': 2, 'ENTP': 3, 'ESFJ': 4, 'ESFP': 5, 'ESTJ': 6, 'ESTP': 7, 'INFJ': 8, 'INFP': 9, 'INTJ': 10, 'INTP': 11, 'ISFJ': 12, 'ISFP': 13, 'ISTJ': 14, 'ISTP': 15}\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['type'])\n",
    "\n",
    "# Store label encoding mapping for future reference\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data save\n",
    "data_to_save = pd.DataFrame({'Processed_Posts': X, 'Labels': y})\n",
    "data_to_save.to_csv('../data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Manipulate Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed_data.csv')\n",
    "\n",
    "# Extract features and target\n",
    "X = data['Processed_Posts']\n",
    "y = data['Labels']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels\n",
       "9     1462\n",
       "8     1182\n",
       "11    1011\n",
       "10     898\n",
       "1      550\n",
       "3      550\n",
       "15     270\n",
       "13     218\n",
       "2      187\n",
       "14     161\n",
       "0      149\n",
       "12     121\n",
       "7       74\n",
       "5       40\n",
       "4       35\n",
       "6       32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Normalization by TF-IDF & SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data to numerical format using TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)      \n",
    "\n",
    "# Address class imbalance in the training set using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels\n",
       "8     1462\n",
       "1     1462\n",
       "3     1462\n",
       "9     1462\n",
       "0     1462\n",
       "10    1462\n",
       "2     1462\n",
       "11    1462\n",
       "4     1462\n",
       "13    1462\n",
       "15    1462\n",
       "12    1462\n",
       "6     1462\n",
       "7     1462\n",
       "14    1462\n",
       "5     1462\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train_smote).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.6870317002881844\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.37      0.46        41\n",
      "           1       0.69      0.66      0.67       125\n",
      "           2       0.57      0.48      0.52        44\n",
      "           3       0.65      0.62      0.63       135\n",
      "           4       0.25      0.29      0.27         7\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.50      0.43      0.46         7\n",
      "           7       0.78      0.47      0.58        15\n",
      "           8       0.71      0.68      0.69       288\n",
      "           9       0.69      0.80      0.74       370\n",
      "          10       0.64      0.70      0.67       193\n",
      "          11       0.72      0.78      0.75       293\n",
      "          12       0.90      0.62      0.74        45\n",
      "          13       0.64      0.53      0.58        53\n",
      "          14       0.79      0.52      0.63        44\n",
      "          15       0.71      0.63      0.67        67\n",
      "\n",
      "    accuracy                           0.69      1735\n",
      "   macro avg       0.62      0.54      0.57      1735\n",
      "weighted avg       0.69      0.69      0.68      1735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy: 0.47723342939481267\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.05      0.09        41\n",
      "           1       0.64      0.29      0.40       125\n",
      "           2       0.56      0.11      0.19        44\n",
      "           3       0.60      0.29      0.39       135\n",
      "           4       0.00      0.00      0.00         7\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.00      0.00      0.00        15\n",
      "           8       0.41      0.59      0.49       288\n",
      "           9       0.42      0.81      0.55       370\n",
      "          10       0.54      0.41      0.46       193\n",
      "          11       0.59      0.61      0.60       293\n",
      "          12       1.00      0.04      0.09        45\n",
      "          13       0.83      0.09      0.17        53\n",
      "          14       0.50      0.05      0.08        44\n",
      "          15       0.75      0.18      0.29        67\n",
      "\n",
      "    accuracy                           0.48      1735\n",
      "   macro avg       0.47      0.22      0.24      1735\n",
      "weighted avg       0.53      0.48      0.44      1735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Accuracy: 0.6501440922190201\n",
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.20      0.30        41\n",
      "           1       0.66      0.60      0.63       125\n",
      "           2       0.53      0.41      0.46        44\n",
      "           3       0.59      0.61      0.60       135\n",
      "           4       0.50      0.14      0.22         7\n",
      "           5       1.00      0.12      0.22         8\n",
      "           6       0.50      0.14      0.22         7\n",
      "           7       0.75      0.40      0.52        15\n",
      "           8       0.63      0.66      0.65       288\n",
      "           9       0.65      0.83      0.73       370\n",
      "          10       0.62      0.66      0.64       193\n",
      "          11       0.72      0.70      0.71       293\n",
      "          12       0.84      0.47      0.60        45\n",
      "          13       0.67      0.49      0.57        53\n",
      "          14       0.63      0.50      0.56        44\n",
      "          15       0.63      0.55      0.59        67\n",
      "\n",
      "    accuracy                           0.65      1735\n",
      "   macro avg       0.66      0.47      0.51      1735\n",
      "weighted avg       0.65      0.65      0.64      1735\n",
      "\n",
      "SVM - Accuracy: 0.669164265129683\n",
      "Classification Report for SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.34      0.46        41\n",
      "           1       0.68      0.63      0.65       125\n",
      "           2       0.61      0.43      0.51        44\n",
      "           3       0.64      0.61      0.62       135\n",
      "           4       0.40      0.29      0.33         7\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.50      0.14      0.22         7\n",
      "           7       0.78      0.47      0.58        15\n",
      "           8       0.68      0.67      0.68       288\n",
      "           9       0.65      0.80      0.72       370\n",
      "          10       0.61      0.70      0.65       193\n",
      "          11       0.70      0.78      0.74       293\n",
      "          12       0.95      0.47      0.63        45\n",
      "          13       0.70      0.43      0.53        53\n",
      "          14       0.76      0.43      0.55        44\n",
      "          15       0.75      0.60      0.67        67\n",
      "\n",
      "    accuracy                           0.67      1735\n",
      "   macro avg       0.63      0.49      0.53      1735\n",
      "weighted avg       0.67      0.67      0.66      1735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    'SVM': SVC(kernel='linear', random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} - Accuracy: {accuracy}\")\n",
    "    print(f\"Classification Report for {name}:\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tuning Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Use RandomsearchCV for hyper-parameter tuning\n",
    "- 80% for model training.\n",
    "- 20% for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 2/3] END C=0.3811544088653064, penalty=l2, solver=lbfgs;, score=0.910 total time=  46.9s\n",
      "[CV 1/3] END C=0.0018794668241638478, penalty=l2, solver=liblinear;, score=0.740 total time=   6.4s\n",
      "[CV 3/3] END C=0.0018794668241638478, penalty=l2, solver=liblinear;, score=0.735 total time=   5.7s\n",
      "[CV 2/3] END ....penalty=None, solver=newton-cg;, score=0.938 total time= 2.3min\n",
      "[CV 1/3] END C=0.006690421166498805, penalty=l2, solver=lbfgs;, score=0.743 total time=   8.8s\n",
      "[CV 2/3] END C=0.006690421166498805, penalty=l2, solver=lbfgs;, score=0.817 total time=   7.6s\n",
      "[CV 3/3] END C=0.006690421166498805, penalty=l2, solver=lbfgs;, score=0.774 total time=   6.9s\n",
      "[CV 1/3] END C=0.039054412752107935, penalty=l2, solver=newton-cg;, score=0.776 total time=  15.4s\n",
      "[CV 2/3] END C=0.039054412752107935, penalty=l2, solver=newton-cg;, score=0.851 total time=  13.7s\n",
      "[CV 3/3] END C=0.039054412752107935, penalty=l2, solver=newton-cg;, score=0.824 total time=  15.0s\n",
      "[CV 1/3] END C=0.3811544088653064, penalty=l2, solver=lbfgs;, score=0.857 total time=  48.2s\n",
      "[CV 2/3] END C=0.0018794668241638478, penalty=l2, solver=liblinear;, score=0.797 total time=   6.1s\n",
      "[CV 1/3] END ....penalty=None, solver=newton-cg;, score=0.922 total time= 1.5min\n",
      "[CV 3/3] END ....penalty=None, solver=newton-cg;, score=0.970 total time= 2.2min\n",
      "[CV 1/3] END C=6.015308718396457, l1_ratio=0.7222222222222222, penalty=elasticnet, solver=saga;, score=0.878 total time= 6.1min\n",
      "[CV 2/3] END C=6.015308718396457, l1_ratio=0.7222222222222222, penalty=elasticnet, solver=saga;, score=0.924 total time= 6.5min\n",
      "[CV 3/3] END C=6.015308718396457, l1_ratio=0.7222222222222222, penalty=elasticnet, solver=saga;, score=0.959 total time= 6.5min\n",
      "[CV 3/3] END C=0.3811544088653064, penalty=l2, solver=lbfgs;, score=0.931 total time=  45.5s\n",
      "[CV 3/3] END .........penalty=None, solver=saga;, score=0.971 total time= 9.7min\n",
      "[CV 1/3] END C=0.00039796923010559904, l1_ratio=0.7222222222222222, penalty=elasticnet, solver=saga;, score=0.062 total time=   4.5s\n",
      "[CV 1/3] END C=0.4042872735027334, penalty=l2, solver=saga;, score=0.860 total time=  15.7s\n",
      "[CV 3/3] END C=0.4042872735027334, penalty=l2, solver=saga;, score=0.933 total time=  15.7s\n",
      "[CV 2/3] END .........penalty=None, solver=saga;, score=0.939 total time=10.2min\n",
      "[CV 2/3] END C=0.00039796923010559904, l1_ratio=0.7222222222222222, penalty=elasticnet, solver=saga;, score=0.063 total time=   4.1s\n",
      "[CV 3/3] END C=0.00039796923010559904, l1_ratio=0.7222222222222222, penalty=elasticnet, solver=saga;, score=0.062 total time=   1.6s\n",
      "[CV 2/3] END C=0.4042872735027334, penalty=l2, solver=saga;, score=0.912 total time=  14.7s\n",
      "[CV 1/3] END C=0.00013289448722869195, l1_ratio=0.18888888888888888, penalty=elasticnet, solver=saga;, score=0.062 total time=   2.3s\n",
      "[CV 2/3] END C=0.00013289448722869195, l1_ratio=0.18888888888888888, penalty=elasticnet, solver=saga;, score=0.062 total time=   9.8s\n",
      "[CV 3/3] END C=0.00013289448722869195, l1_ratio=0.18888888888888888, penalty=elasticnet, solver=saga;, score=0.062 total time=   2.5s\n",
      "[CV 1/3] END .........penalty=None, solver=saga;, score=0.922 total time=10.8min\n",
      "Best Parameters found:  {'penalty': None, 'solver': 'saga'}\n",
      "Test Accuracy: 0.6645533141210375\n",
      "Test Precision: 0.6126843789384688\n",
      "Test Recall: 0.5088798795487397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacksonzhao/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define parameter\n",
    "param_dist = [\n",
    "    {'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], 'penalty': ['l2'], 'C': loguniform(1e-4, 1e+2)},\n",
    "    {'solver': ['liblinear'], 'penalty': ['l1', 'l2'], 'C': loguniform(1e-4, 1e+2)},\n",
    "    {'solver': ['saga'], 'penalty': ['elasticnet'], 'C': loguniform(1e-4, 1e+2), 'l1_ratio': np.linspace(0.1, 0.9, 10)},\n",
    "    {'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], 'penalty': [None]} \n",
    "]\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Setup RandomizedSearchCV with verbose output\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, scoring='accuracy', n_jobs=-1, cv=3, random_state=42, verbose=3)\n",
    "\n",
    "# Fit the RandomizedSearchCV to find the best parameters\n",
    "random_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters found: \", random_search.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions on the test set\n",
    "y_pred_test = random_search.best_estimator_.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate and print accuracy, precision, and recall for the test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test, average='macro')\n",
    "recall_test = recall_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_test}\")\n",
    "print(f\"Test Precision: {precision_test}\")\n",
    "print(f\"Test Recall: {recall_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Use best tunned parameter to test whole labels\n",
    "- full dataset for y-labels\n",
    "- save the best parameter for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, penalty=None, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, penalty=None, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, penalty=None, solver='saga')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use TF-IDF and Smote for retraining process\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X) \n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_tfidf, y)\n",
    "\n",
    "# Best parameters as found from tuning\n",
    "best_params = {'penalty': None, 'solver': 'saga'} \n",
    "\n",
    "# Initialize and train the Logistic Regression model using the best parameters on the entire dataset\n",
    "model = LogisticRegression(**best_params, max_iter=1000)\n",
    "model.fit(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Save to the output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved to '../output/best_model_logistic_regression.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save this best model to a pickle file\n",
    "with open('../output/best_model_logistic_regression.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(\"The best model was saved to '../output/best_model_logistic_regression.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extract Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: feature balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing data by SMOTE\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the models with best parameter\n",
    "filename = '../output/best_model_logistic_regression.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: label manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up mapping between label indices and label names\n",
    "coef = model.coef_\n",
    "\n",
    "# mapping labels to names\n",
    "map_label = {}\n",
    "for key, val in label_mapping.items():\n",
    "  map_label[val] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Visualization of feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "# only present english sentiment words\n",
    "def is_english_word(word):\n",
    "    return word.isalpha() and word.lower() in english_words\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "coefficients = model.coef_\n",
    "\n",
    "for class_index in range(coefficients.shape[0]):\n",
    "    class_coeffs = coefficients[class_index]\n",
    "    sorted_indices = class_coeffs.argsort()[::-1]\n",
    "    class_name = map_label[class_index]\n",
    "    \n",
    "    # Exclude label-specific features and filter non-English or meaningless words\n",
    "    filtered_features = [(feature_names[i], class_coeffs[i]) for i in sorted_indices\n",
    "                         if class_name.lower() not in feature_names[i].lower() and is_english_word(feature_names[i])]\n",
    "\n",
    "    # Select the top 15 meaningful English words\n",
    "    top_features = filtered_features[:15]\n",
    "    features, importances = zip(*top_features)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    bars = plt.bar(features, importances, color='skyblue')\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, round(yval, 2), ha='center', va='bottom')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title(f\"Top Features for Class '{class_name}'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4: restore all feature importance for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features for class ENFJ saved to '../output/all_features_for_ENFJ.csv'\n",
      "All features for class ENFP saved to '../output/all_features_for_ENFP.csv'\n",
      "All features for class ENTJ saved to '../output/all_features_for_ENTJ.csv'\n",
      "All features for class ENTP saved to '../output/all_features_for_ENTP.csv'\n",
      "All features for class ESFJ saved to '../output/all_features_for_ESFJ.csv'\n",
      "All features for class ESFP saved to '../output/all_features_for_ESFP.csv'\n",
      "All features for class ESTJ saved to '../output/all_features_for_ESTJ.csv'\n",
      "All features for class ESTP saved to '../output/all_features_for_ESTP.csv'\n",
      "All features for class INFJ saved to '../output/all_features_for_INFJ.csv'\n",
      "All features for class INFP saved to '../output/all_features_for_INFP.csv'\n",
      "All features for class INTJ saved to '../output/all_features_for_INTJ.csv'\n",
      "All features for class INTP saved to '../output/all_features_for_INTP.csv'\n",
      "All features for class ISFJ saved to '../output/all_features_for_ISFJ.csv'\n",
      "All features for class ISFP saved to '../output/all_features_for_ISFP.csv'\n",
      "All features for class ISTJ saved to '../output/all_features_for_ISTJ.csv'\n",
      "All features for class ISTP saved to '../output/all_features_for_ISTP.csv'\n"
     ]
    }
   ],
   "source": [
    "for class_index in range(coefficients.shape[0]):\n",
    "    class_coeffs = coefficients[class_index]\n",
    "    sorted_indices = class_coeffs.argsort()[::-1]\n",
    "    class_name = map_label[class_index]\n",
    "    \n",
    "    # Extract all features for this class\n",
    "    all_features = [(feature_names[i], class_coeffs[i]) for i in sorted_indices]\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(all_features, columns=['Feature', 'Importance'])\n",
    "    df.to_csv(f'../output/all_features_for_{class_name}.csv', index=False)\n",
    "\n",
    "    print(f\"All features for class {class_name} saved to '../output/all_features_for_{class_name}.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
